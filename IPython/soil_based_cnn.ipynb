{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#    Author: Alan Bauer, alan.bauer@earlham.ac.uk               #\n",
    "#    Date: 2018.11.15, V0.1                                     #\n",
    "#    Version: 0.1                                               #\n",
    "#                                                               #\n",
    "#################################################################\n",
    "\n",
    "# Attempting to segment aerial wheat images into individual plots\n",
    "# and extract data from each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "import csv\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import sgd\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "import keras\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from skimage.measure import label, regionprops, shannon_entropy\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn\n",
      "env: KMP_DUPLICATE_LIB_OK=TRUE\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "%set_env KMP_DUPLICATE_LIB_OK=TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process into 9x9 images. It assumes that if they aren't 9x9 they are 11x11\n",
    "# because that is the size I first started classifying. In that case the\n",
    "# outermost pixels are removed so the center pixel remains the same.\n",
    "def preprocess(img):\n",
    "    h,w = img.shape[:2]\n",
    "    if h is 9 and w is 9:\n",
    "        return img\n",
    "    \n",
    "    up_img = img[1:10,1:10]\n",
    "\n",
    "    return up_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just testing that the preprocess function properly resizes images\n",
    "#img = \"datasets/soil/soil/0_0_04_28.png\"\n",
    "#img = cv2.imread(img)\n",
    "#up_img = preprocess(img)\n",
    "#cv2.imwrite(\"test.png\",up_img)\n",
    "#print(up_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from a base directory that contains two subfolders,\n",
    "# one for each of the two classes\n",
    "def load_imgs(base_path):\n",
    "    pos_class = \"soil\"\n",
    "    neg_class = \"not_soil\"\n",
    "    \n",
    "    files = []\n",
    "    ext = \".png\" # May need to make this more comprehensive on a different OS\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for (top_dir, dirs, filenames) in os.walk(base_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(ext):\n",
    "                f = os.path.join(top_dir,filename)\n",
    "                img = cv2.imread(f)\n",
    "                img = preprocess(img)\n",
    "                data.append(img)\n",
    "\n",
    "                label = f.split(os.path.sep)[-2]\n",
    "                labels.append(label)\n",
    "    \n",
    "    return (np.array(data), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN\n",
    "def build_model(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    shape = (height, width, depth)\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        shape = (depth, height, width)\n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", activation=\"relu\", input_shape=shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(classes,activation=\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and split it into training and testing sets\n",
    "data_dir = \"datasets/soil/\"\n",
    "#(data, labels) = load_imgs(data_dir)\n",
    "#data = data.astype(\"float\") / 255.0\n",
    "\n",
    "#(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=1)\n",
    "\n",
    "#trainY = LabelBinarizer().fit_transform(trainY)\n",
    "#testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "#trainY = np_utils.to_categorical(trainY)\n",
    "#testY = np_utils.to_categorical(testY)\n",
    "\n",
    "#print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = build_model(9,9,3,2)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network\n",
      "Train on 2748 samples, validate on 916 samples\n",
      "Epoch 1/100\n",
      "2748/2748 [==============================] - 12s 4ms/step - loss: 0.2922 - acc: 0.9079 - val_loss: 0.3342 - val_acc: 0.8897\n",
      "Epoch 2/100\n",
      "2748/2748 [==============================] - 10s 3ms/step - loss: 0.1960 - acc: 0.9385 - val_loss: 0.3746 - val_acc: 0.8941\n",
      "Epoch 3/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.1548 - acc: 0.9487 - val_loss: 0.2915 - val_acc: 0.9159\n",
      "Epoch 4/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.1249 - acc: 0.9592 - val_loss: 0.0752 - val_acc: 0.9760\n",
      "Epoch 5/100\n",
      "2748/2748 [==============================] - 8s 3ms/step - loss: 0.1218 - acc: 0.9556 - val_loss: 0.1930 - val_acc: 0.9531\n",
      "Epoch 6/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.1133 - acc: 0.9643 - val_loss: 0.1179 - val_acc: 0.9651\n",
      "Epoch 7/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0904 - acc: 0.9709 - val_loss: 0.1751 - val_acc: 0.9531\n",
      "Epoch 8/100\n",
      "2748/2748 [==============================] - 8s 3ms/step - loss: 0.0858 - acc: 0.9698 - val_loss: 0.1138 - val_acc: 0.9694\n",
      "Epoch 9/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0751 - acc: 0.9745 - val_loss: 0.0966 - val_acc: 0.9672\n",
      "Epoch 10/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0783 - acc: 0.9760 - val_loss: 0.0870 - val_acc: 0.9738\n",
      "Epoch 11/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0605 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9716\n",
      "Epoch 12/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0530 - acc: 0.9807 - val_loss: 0.0889 - val_acc: 0.9749\n",
      "Epoch 13/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0544 - acc: 0.9803 - val_loss: 0.1247 - val_acc: 0.9531\n",
      "Epoch 14/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0434 - acc: 0.9869 - val_loss: 0.2088 - val_acc: 0.9465\n",
      "Epoch 15/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0451 - acc: 0.9869 - val_loss: 0.1844 - val_acc: 0.9498\n",
      "Epoch 16/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0544 - acc: 0.9807 - val_loss: 0.0927 - val_acc: 0.9716\n",
      "Epoch 17/100\n",
      "2748/2748 [==============================] - 8s 3ms/step - loss: 0.0450 - acc: 0.9844 - val_loss: 0.0839 - val_acc: 0.9771\n",
      "Epoch 18/100\n",
      "2112/2748 [======================>.......] - ETA: 1s - loss: 0.0426 - acc: 0.9853"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-280769138216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] evaluating network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model, if you are loading a model from disk, skip this step\n",
    "print(\"[INFO] training network\")\n",
    "\n",
    "H = model.fit(trainX, trainY, validation_data=(testX,testY),batch_size=32, epochs=100, verbose=1)\n",
    "\n",
    "print(\"[INFO] evaluating network\")\n",
    "predictions = model.predict(testX, batch_size=32,verbose=1)\n",
    "target_names = [\"not soil\",\"soil\"]\n",
    "print(classification_report(testY.argmax(axis=1),predictions.argmax(axis=1),target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or load model\n",
    "\n",
    "#model.save(\"models/soil/model_5.h5\")\n",
    "model = load_model(\"model_5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 32)          896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,146,306\n",
      "Trainable params: 1,144,834\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Info about model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extracts 9x9 images from a larger image and returns\n",
    "# 4 numpy arrays, the images, an empty array for labels to be added\n",
    "# and the coordinates of the top left corner of the images\n",
    "def get_small_imgs_from_mosaic(img):\n",
    "    h,w = img.shape[:2]\n",
    "    step = 6\n",
    "    size = 9\n",
    "        \n",
    "    step_x = 5\n",
    "    step_y = 5\n",
    "    \n",
    "    images_to_pred = []\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    for x in range(0,w-size,step):\n",
    "        for y in range(0,h-size,step):\n",
    "            small_img = img[y:y+size,x:x+size]\n",
    "            small_img = small_img.astype(\"float\") / 255.0\n",
    "            images_to_pred.append(small_img)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    \n",
    "    labels = [None] * len(X)\n",
    "    \n",
    "    return (np.array(images_to_pred),np.array(labels),np.array(X),np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Courtesy of pyimagesearch.com\n",
    "def order_points(pts):\n",
    "    rect = np.zeros((4,2), dtype=\"float32\")\n",
    "    \n",
    "    s = []\n",
    "    for i in range(len(pts)):\n",
    "        s.append(pts[i][0] + pts[i][1])\n",
    "        \n",
    "    rect[0] = pts[np.argmin(s)]\n",
    "    rect[2] = pts[np.argmax(s)]\n",
    "    \n",
    "    diff = np.diff(pts, axis=1)\n",
    "    rect[3] = pts[np.argmin(diff)]\n",
    "    rect[1] = pts[np.argmax(diff)]\n",
    "    \n",
    "    return rect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_point_transform(image,pts):\n",
    "    rect = order_points(pts)\n",
    "    (tl, tr, br, bl) = rect\n",
    "    #print(\"4points\")\n",
    "    #print(rect)\n",
    "    \n",
    "    widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\n",
    "    widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\n",
    "    maxWidth = max(int(widthA),int(widthB))\n",
    "    #print(\"maxW = \" + str(maxWidth))\n",
    "    \n",
    "    heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\n",
    "    heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\n",
    "    maxHeight = max(int(heightA), int(heightB))\n",
    "    #print(\"maxH = \" + str(maxHeight))\n",
    "    \n",
    "    dst = np.array([\n",
    "        [0,0],\n",
    "        [0, maxWidth],\n",
    "        [maxHeight, maxWidth],\n",
    "        [maxHeight, 0]\n",
    "    ], dtype = \"float32\")\n",
    "    #print(rect)\n",
    "    #print(dst)\n",
    "    M = cv2.getPerspectiveTransform(rect,dst)\n",
    "    #print(M)\n",
    "    M = np.array([\n",
    "        [1.03645, 0.01255, -84.5],\n",
    "        [-0.01022,1.11267,-136.5],\n",
    "        [-0.00,0,1]], dtype=\"float64\")\n",
    "    warped = cv2.warpPerspective(image, M, (maxWidth,maxHeight))#,flags=cv2.WARP_INVERSE_MAP)\n",
    "        \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dots(img):\n",
    "    h,w = img.shape[:2]\n",
    "    mask = np.zeros((h,w)).astype('uint8')\n",
    "    #mask[img[:,:]==[0,0,255]] = 255\n",
    "    for y in range(h):\n",
    "        for x in range(w):\n",
    "            if img[y,x,0] == 0 and img[y,x,1] == 0 and img[y,x,2] == 255:\n",
    "                mask[y,x] = 255\n",
    "    \n",
    "    # We have a mask with the four red dots white and everything else black.\n",
    "    labels = label(mask)\n",
    "    regions = regionprops(labels)\n",
    "    \n",
    "    dots = []\n",
    "    for region in regions:\n",
    "        dots.append(region['centroid'])\n",
    "\n",
    "    cv2.imwrite(\"mask.png\",mask)    \n",
    "\n",
    "    return dots\n",
    "    #print(dots)\n",
    "    \n",
    "    #warped = four_point_transform(img,dots)\n",
    "\n",
    "    #cv2.imwrite(\"warped.png\", warped)\n",
    "    #return warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/air_surf_wheat_cnn/20mb_imgs/ChurchFarm-180613-DFW_roi.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/ChurchFarm-180613-DFW_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/ChurchFarm-180625-DFW_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/ChurchFarm-180702-DFW_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/ChurchFarm-180709-DFW_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/ChurchFarm-180716-DFW_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/ChurchFarm-180723-DFW_roi.png\")\n",
    "\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/air_surf_wheat_cnn/rothamsted_research/RRes_180613_Ortho.png\")\n",
    "\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/CF_DFW_13062018_Flattened_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/CF_DFW_25062018_Flattened_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/CF_DFW_02072018_Flattened_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/CF_DFW_09072018_Flattened_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/CF_DFW_16072018_Flattened_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/20mb_imgs/CF_DFW_23072018_Flattened_roi.png\")\n",
    "\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/rothamsted_research/RRes_180410_Ortho_roi.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/rothamsted_research/RRes_180515_Ortho_roi_small.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/rothamsted_research/RRes_180613_Ortho_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/rothamsted_research/RRes_180410_P4D_Flattened_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/rothamsted_research/RRes_180515_Flattened_roi.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/air_surf_wheat_cnn/rothamsted_research/RRes_180613_P4D_Flattened_roi.png\")\n",
    "\n",
    "\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_04_28.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_05_06.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_05_13.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_05_19.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_05_23.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_05_26.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_06_03.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_06_10.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_06_22.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_07_01.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_07_07.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_07_13.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_07_19.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_07_22.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_07_26.png\")\n",
    "#hmap = cv2.imread(\"/Users/bauera/work/airsurf/wheat/2016_field/2016_08_02.png\")\n",
    "\n",
    "img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/DFW_Early_2019/19_05_29/DFW_Early_190529_transformed_small.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/DFW_Early_2019/19_05_29/DFW_Early_190529_transformed_med.png\")\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/wheat/DFW_Early_2019/19_05_29/DFW_Early_190529_transformed_large.png\")\n",
    "\n",
    "\n",
    "if img.shape[2] > 3:\n",
    "    img = img[:,:,:3]\n",
    "\n",
    "img_h, img_w = img.shape[:2]\n",
    "#hmap_h, hmap_w = hmap.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2205, 4500, 3)\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warped = four_point_transform(img, dots)\n",
    "#cv2.imwrite(\"warped2.png\", warped)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dots = [(122.77324632952691, 72.97879282218597),\n",
    "#        (165.55772357723578, 4559.160975609756),\n",
    "#        (2224.431818181818, 57.63474025974026),\n",
    "#        (2264.3284789644013, 4536.747572815534)]\n",
    "\n",
    "#print(dots)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274134\n"
     ]
    }
   ],
   "source": [
    "# What images should we read in for segmentation\n",
    "#img = cv2.imread(\"/Users/bauera/work/airsurf/air_surf_wheat_cnn/20mb_imgs/ChurchFarm-180613-DFW.png\")\n",
    "#img = cv2.imread(\"20mb_pngs/Morley_180611.png\")\n",
    "(images, labels, x_values, y_values) = get_small_imgs_from_mosaic(img)\n",
    "\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274134/274134 [==============================] - 94s 344us/step\n"
     ]
    }
   ],
   "source": [
    "# Run the model on the small images extracted from the original\n",
    "outputs = model.predict(images,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draws the areas classified as soil with >98% confidence on\n",
    "# the original image as well as a black and white mask and\n",
    "# saves them to disk\n",
    "h,w = img.shape[:2]\n",
    "size = 9\n",
    "step = 6\n",
    "out_img = img.copy()\n",
    "out_img_bw = np.zeros((h,w))\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    if outputs[i][1] >= 0.98:\n",
    "        x = x_values[i]\n",
    "        y = y_values[i]\n",
    "        cv2.rectangle(out_img, (x+1,y+1),(x+size-1,y+size-1),(0,0,255),-1)\n",
    "        cv2.rectangle(out_img_bw,(x+1,y+1),(x+size-1,y+size-1),255,-1)\n",
    "        #out_img_bw[y+1:y+size-1][x+1:x+size-1] = 1\n",
    "        #out_img_bw[x+1:x+size-1][y+1:y+size-1] = 1\n",
    "        \n",
    "cv2.imwrite(\"output.png\",out_img)\n",
    "\n",
    "out_img_bw = out_img_bw * 255\n",
    "out_img_bw.astype(\"uint8\")\n",
    "#kernel = np.ones((11,11))\n",
    "#out_img_bw =cv2.dilate(out_img_bw,kernel,1)\n",
    "#out_img_bw = cv2.morphologyEx(out_img_bw, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imwrite(\"output_bw.png\",out_img_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines lines that are too close together, ideally\n",
    "# resulting in only 1 line between plots.\n",
    "def line_consensus(lines):\n",
    "    lines.sort()\n",
    "    cons = []\n",
    "    #print(lines)\n",
    "\n",
    "    for i in range(len(lines)-1):\n",
    "        if lines[i+1] - lines[i] < 10:\n",
    "            avg = (lines[i+1] + lines[i]) / 2\n",
    "            cons.append(avg)\n",
    "        # Special case for item first in list\n",
    "        elif lines[i] - lines[i-1] >= 10 and i != 0:\n",
    "            #continue\n",
    "            cons.append(lines[i])\n",
    "        elif i == 0:\n",
    "            cons.append(lines[i])\n",
    "    \n",
    "    # Special case for item last in list\n",
    "    if lines[-1] - lines[-2] >= 10:\n",
    "        cons.append(lines[-1])\n",
    "    \n",
    "    return cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a mask image from the x and y coordinates of\n",
    "# vertical and horizontal lines, respectively.\n",
    "def mask_write(shape,ver_cons,hor_cons,name):\n",
    "    mask = np.zeros(shape)\n",
    "    \n",
    "    # Vertical lines\n",
    "    for line in ver_cons:\n",
    "        x = int(line)\n",
    "        y0 = 0\n",
    "        y1 = shape[0]\n",
    "        cv2.line(mask,(x,y0),(x,y1),255,1)\n",
    "    \n",
    "    for line in hor_cons:\n",
    "        y = int(line)\n",
    "        x0 = 0\n",
    "        x1 = shape[1]\n",
    "        cv2.line(mask,(x0,y),(x1,y),255,1)\n",
    "    \n",
    "    cv2.imwrite(name,mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the fields have small plots that make it so the\n",
    "# lines between plots are not uniformly spaced horizontally\n",
    "# (on these images, it could work the other way too) I\n",
    "# wanted to see if finding the edge lines and then simply\n",
    "# equalizing the distance between all the intervening lines\n",
    "# would work. It didn't. Either the plots are not exactly\n",
    "# the same width, or pixel width is not uniform across the\n",
    "# entire image.\n",
    "def vert_equalize(lines):\n",
    "    distances = []\n",
    "    ret_lines = []\n",
    "    for i in range(len(lines)-1):\n",
    "        distances.append(lines[i+1] - lines[i])\n",
    "    \n",
    "    print(np.mean(distances))\n",
    "    mean = np.mean(distances)\n",
    "    least = lines[0]\n",
    "    most = lines[-1]\n",
    "    for i in range(len(lines)):\n",
    "        if i == 0:\n",
    "            ret_lines.append(least)\n",
    "        elif i == len(lines)-1:\n",
    "            ret_lines.append(most)\n",
    "        else:\n",
    "            ret_lines.append(least + mean * i)\n",
    "    \n",
    "    return ret_lines\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw all the Hough Lines on the image.\n",
    "# Save a black and white mask, and the original\n",
    "# with lines drawn on it.\n",
    "out = img.copy()\n",
    "#cv2.imwrite(\"test.png\",out)\n",
    "### Transition to using Hough lines\n",
    "#out_img_bw = cv2.imread(\"output_bw.png\")\n",
    "mask = out_img_bw\n",
    "hough_bw = np.zeros((h,w))\n",
    "#edges = cv2.Canny(out_img,50,150,aperture_size=3)\n",
    "lines = cv2.HoughLines(mask.astype(\"uint8\"),0.1,np.pi/90,700)\n",
    "#print(lines.shape)\n",
    "counts = 0\n",
    "for line in lines:\n",
    "    if line[0][1] > 0.01 and line[0][1] < 1.55:\n",
    "        continue\n",
    "    if line[0][1] > 1.58:\n",
    "        continue\n",
    "    #print(line)\n",
    "    for rho,theta in line:\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a*rho\n",
    "        y0 = b*rho\n",
    "        x1 = int(x0 + 10000*(-b))\n",
    "        y1 = int(y0 + 10000*(a))\n",
    "        x2 = int(x0 - 10000*(-b))\n",
    "        y2 = int(y0 - 10000*(a))\n",
    "    \n",
    "        cv2.line(out,(x1,y1),(x2,y2),(0,0,255),1)\n",
    "        cv2.line(hough_bw,(x1,y1),(x2,y2),255,1)\n",
    "        counts += 1\n",
    "\n",
    "\n",
    "hough_bw = np.bitwise_not(hough_bw.astype(\"uint8\"))\n",
    "print(counts)\n",
    "\n",
    "cv2.imwrite('houghlines.png', out)\n",
    "cv2.imwrite('houghlines_bw.png', hough_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2205, 4500, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the Hough lines into horizontal and vertical lines\n",
    "\n",
    "### I want to combine the nearby lines rather than just making them thicker.\n",
    "hor = []\n",
    "ver = []\n",
    "\n",
    "for line in lines:\n",
    "    if line[0][1] < 0.01:\n",
    "        ver.append(line[0,0])\n",
    "    elif line[0][1] > 1.55 and line[0][1] < 1.58:\n",
    "        hor.append(line[0,0])\n",
    "\n",
    "hor.sort()\n",
    "ver.sort()\n",
    "ver2 = ver\n",
    "hor2 = hor\n",
    "hor_cons = []\n",
    "ver_cons = []\n",
    "\n",
    "while len(hor_cons) != len(hor):\n",
    "    hor_cons = hor\n",
    "    hor = line_consensus(hor)\n",
    "    \n",
    "while len(ver_cons) != len(ver):\n",
    "    ver_cons = ver\n",
    "    ver = line_consensus(ver)\n",
    "    \n",
    "#ver_eq = vert_equalize(ver_cons)\n",
    "#Making the horizontal distances equal is not as accurate, because the plots are not all perfectly the same size.\n",
    "#print(len(ver_cons))\n",
    "#print(len(ver_eq))\n",
    "\n",
    "\n",
    "# Write out various images using different sets of lines.\n",
    "mask = mask_write((h,w),ver_cons,hor_cons,\"test0.png\")\n",
    "mask = np.bitwise_not(mask.astype(\"uint8\"))\n",
    "cv2.imwrite(\"mask2.png\",mask)\n",
    "#mask_write((h,w),ver_cons,hor_cons,\"test.png\") # Redundant with test0\n",
    "mask_write((h,w),ver2,hor2,\"test2.png\")\n",
    "\n",
    "cutout = img.copy()\n",
    "output = cv2.bitwise_and(cutout,cutout,mask=mask)\n",
    "print(cutout.shape)\n",
    "cv2.imwrite(\"test3.png\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2338, 4637, 3)\n",
      "(2338, 4637)\n"
     ]
    }
   ],
   "source": [
    "# Write an image that should only include the areas that are in plots, but will also include small plots.\n",
    "cutout = img.copy()\n",
    "\n",
    "output = cv2.bitwise_and(cutout,cutout,mask=hough_bw)\n",
    "print(cutout.shape)\n",
    "print(hough_bw.shape)\n",
    "cv2.imwrite(\"cutout.png\",output)\n",
    "mask = hough_bw\n",
    "\n",
    "#b,g,r = cv2.split(img)\n",
    "#b = b & hough_bw\n",
    "#g = g & hough_bw\n",
    "#r = r & hough_bw\n",
    "\n",
    "#cutouts = cv2.merge(b,g)\n",
    "#cutouts = cv2.merge(cutouts,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714\n",
      "714\n"
     ]
    }
   ],
   "source": [
    "### Extract plots  49x16\n",
    "# I was using hard-coded values to see about how big the area of each plot is\n",
    "# but it should be detected automatically, using the numbers of rows\n",
    "# and columns.\n",
    "plots = []\n",
    "\n",
    "im2,contours,hierarchy = cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for contour in contours:\n",
    "    x,y,width,height = cv2.boundingRect(contour)\n",
    "    area = width*height\n",
    "    plots.append((x,y,width,height,area))\n",
    "\n",
    "areas = []\n",
    "for plot in plots:\n",
    "    areas.append(plot[4])\n",
    "\n",
    "area_set = list(set(areas))\n",
    "hist = []\n",
    "#print(area_set)\n",
    "\n",
    "for i in area_set:\n",
    "    count = 0\n",
    "    for area in areas:\n",
    "        if area == i:\n",
    "            count += 1\n",
    "    \n",
    "    hist.append(count)\n",
    "\n",
    "#plt.bar(hist,area_set)\n",
    "#plt.bar(area_set,hist)\n",
    "#plt.show()\n",
    "\n",
    "# 2016 dimensions\n",
    "#rows = 16\n",
    "#cols = 49\n",
    "\n",
    "# DFW dimensions\n",
    "#rows = 34\n",
    "#cols = 21\n",
    "\n",
    "#Rothamsted dimensions\n",
    "#rows = 6\n",
    "#cols = 60\n",
    "num_plots = rows * cols\n",
    "plot2 = []\n",
    "\n",
    "plot2 = [plot for plot in plots if plot[4] > 4000] # ~5000 for 20mb imgs, 35000 for the 300mb ones\n",
    "print(len(plot2))\n",
    "\n",
    "plot2 = [plot for plot in plot2 if plot[4] < 80000]\n",
    "print(len(plot2))\n",
    "#49*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create images that are the final mask, as well as the original image\n",
    "# with all the non-plot regions removed.\n",
    "final_mask = np.zeros(hough_bw.shape)\n",
    "for plot in plot2:\n",
    "    final_mask[plot[1]:plot[1]+plot[3],plot[0]:plot[0]+plot[2]] = 1\n",
    "\n",
    "\n",
    "plots_img = cv2.bitwise_and(cutout,cutout,mask=final_mask.astype(\"uint8\"))    \n",
    "\n",
    "cv2.imwrite(\"plots.png\",plots_img)\n",
    "#cv2.imwrite(\"final_mask.png\",final_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see if I can overlay plots on heightmap\n",
    "# plot is (x,y,w,h,a)\n",
    "hmap_mask = np.zeros((hmap_h,hmap_w))\n",
    "hmap_plots = []\n",
    "for plot in plot2:\n",
    "    (x,y,w,h,a) = plot\n",
    "    x = float(x) / float(img_w)\n",
    "    x = int(x * hmap_w)\n",
    "    y = float(y) / float(img_h)\n",
    "    y = int(y * hmap_h)\n",
    "    w = float(w) / float(img_w)\n",
    "    w = int(w * hmap_w)\n",
    "    h = float(h) / float(img_h)\n",
    "    h = int(h * hmap_h)\n",
    "    a = w * h\n",
    "    hmap_mask[y:y+h,x:x+w] = 1\n",
    "    hmap_plots.append((x,y,w,h,a))\n",
    "\n",
    "hmap_final = cv2.bitwise_and(hmap,hmap,mask=hmap_mask.astype(\"uint8\"))\n",
    "cv2.imwrite(\"heightmap_plots.png\",hmap_final)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each plot image in a folder, with the row and column\n",
    "# information in its filename.\n",
    "# TODO: This will be changed to saving each plot in a unique\n",
    "# folder, and then saving images from different days together.\n",
    "\n",
    "dir_name = \"dfw_18_07_09\"\n",
    "#dir_name = \"rres_18_05_15\"\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "    \n",
    "if plot2[0][0] > plot2[-1][0]:\n",
    "    plot2 = list(reversed(plot2))\n",
    "\n",
    "grid_row = 1\n",
    "grid_col = 1\n",
    "raw_y = plot2[0][1]\n",
    "plots_with_grids = []\n",
    "    \n",
    "for plot in plot2:\n",
    "    if plot[1] != raw_y:\n",
    "        grid_row += 1\n",
    "        grid_col = 1\n",
    "        raw_y = plot[1]\n",
    "    name = dir_name + \"/\" + str(grid_row) + \"_\" + str(grid_col) + \".png\"\n",
    "    cv2.imwrite(name,plots_img[plot[1]:plot[1]+plot[3],plot[0]:plot[0]+plot[2]])\n",
    "    plots_with_grids.append((grid_row,grid_col,plot[0],plot[1],plot[2],plot[3],plot[4]))\n",
    "    \n",
    "    grid_col += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a clone of the above cell but with hmap_plots instead. Should refactor into\n",
    "# a function that takes the plots as an argument\n",
    "\n",
    "#dir_name = \"../2016_plots_data/08_02\"\n",
    "#if not os.path.exists(dir_name):\n",
    "#    os.mkdir(dir_name)\n",
    "\n",
    "if hmap_plots[0][0] > hmap_plots[-1][0]:\n",
    "    hmap_plots = list(reversed(hmap_plots))\n",
    "\n",
    "grid_row = 1\n",
    "grid_col = 1\n",
    "raw_y = hmap_plots[0][1]\n",
    "plots_with_grids = []\n",
    "\n",
    "for plot in hmap_plots:\n",
    "    if plot[1] != raw_y:\n",
    "        grid_row += 1\n",
    "        grid_col = 1\n",
    "        raw_y = plot[1]\n",
    "    name = dir_name + \"/\" + str(grid_row) + \"_\" + str(grid_col) + \".png\"\n",
    "    cv2.imwrite(name,hmap_final[plot[1]:plot[1]+plot[3],plot[0]:plot[0]+plot[2]])\n",
    "    plots_with_grids.append((grid_row,grid_col,plot[0],plot[1],plot[2],plot[3],plot[4]))\n",
    "    \n",
    "    grid_col += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1016, 1868, 28, 88, 2464)\n"
     ]
    }
   ],
   "source": [
    "print(plot2[713])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the weighted centroid of the plot based on greenness,\n",
    "# which should be the center of the plot.\n",
    "# We can then extrapolate that information to images later in\n",
    "# a series.\n",
    "\n",
    "# NOT FINISHED\n",
    "def centroid(points):\n",
    "    total = 0\n",
    "    weighted_total = 0\n",
    "    for i in range(len(points)):\n",
    "        weighted_total += points[i]*(i+1)\n",
    "        total += points[i]\n",
    "\n",
    "    mean = total / len(points)\n",
    "    centroid = weighted_total / len(points) / mean\n",
    "    return centroid - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_range(mat, min_val, max_val):\n",
    "    width = max_val - min_val\n",
    "    \n",
    "    if width > 0:\n",
    "        mat -= min_val\n",
    "        mat /= float(width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vegetative_index(img):\n",
    "    h,w = img.shape[:2]\n",
    "    if h == 0 and w == 0:\n",
    "        return\n",
    "    f_img = img.astype(np.float64)/255.0 # Normalize the image\n",
    "    r,g,b = cv2.split(f_img)\n",
    "    total = r + g + b\n",
    "    \n",
    "    r = np.divide(r, total)\n",
    "    g = np.divide(g, total)\n",
    "    b = np.divide(b, total)\n",
    "    \n",
    "    ex_g = 2.0 * g - r - b\n",
    "    ex_r = 1.4 * r - b\n",
    "    veg = ex_g - ex_r\n",
    "    \n",
    "    norm_range(veg, -2.4, 2.0)\n",
    "    \n",
    "    veg = veg * 255.0\n",
    "    veg = veg.astype(np.uint8)\n",
    "    \n",
    "    return np.mean(veg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anisotropy(img):\n",
    "    h,w = img.shape[:2]\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    gray = gray.astype('float64')\n",
    "    \n",
    "    grad_x = np.diff(gray, axis=1)\n",
    "    grad_y = np.diff(gray, axis=0)\n",
    "    \n",
    "    grad_x = grad_x[:h - 1, :]\n",
    "    grad_y = grad_y[:, :w - 1]\n",
    "    \n",
    "    # square the gradients\n",
    "    grad_x2 = np.square(grad_x)\n",
    "    grad_y2 = np.square(grad_y)\n",
    "    \n",
    "    # sum the squared gradients together\n",
    "    grad_xy2 = np.add(grad_x2, grad_y2)\n",
    "    \n",
    "    # square root the summation matrix\n",
    "    norm = np.sqrt(grad_xy2)\n",
    "    \n",
    "    # to handle divide by zero case: set the effect of the gradient to 1/255 when too low\n",
    "    norm[norm < 2] = 255\n",
    "    gx = np.divide(grad_x, norm)\n",
    "    gy = np.divide(grad_y, norm)\n",
    "    nxx = np.multiply(gx, gx)\n",
    "    nxy = np.multiply(gy, gx)\n",
    "    nyy = np.multiply(gy, gy)\n",
    "    xx = np.mean(nxx.flatten())\n",
    "    xy = np.mean(nxy.flatten())\n",
    "    yy = np.mean(nyy.flatten())\n",
    "    # eigenvalues and eigenvector of texture tensor\n",
    "    m = (xx + yy) / 2.0\n",
    "    d = (xx - yy) / 2.0\n",
    "    v = math.sqrt(xy * xy + d * d)\n",
    "    v1 = m + v\n",
    "    v2 = m - v\n",
    "    # direction\n",
    "    tn = - math.atan((v2 - xx) / float(xy))\n",
    "    tn = math.degrees(tn)\n",
    "    # score\n",
    "    score_n = abs((v1 - v2) / 2.0 / float(m))\n",
    "    \n",
    "    return tn, score_n    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coverage(img):\n",
    "    h,w = img.shape[:2]\n",
    "    lab_img = cv2.cvtColor(img,cv2.COLOR_RGB2LAB)\n",
    "    luminance = lab_img[:,:,0]\n",
    "    \n",
    "    mean_lum = np.mean(luminance)\n",
    "    std_lum = np.std(luminance)\n",
    "    \n",
    "    _, binary = cv2.threshold(luminance, mean_lum - std_lum, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    num_pixels = h * w\n",
    "    num_pix_nonzero = len(np.flatnonzero(binary))\n",
    "    \n",
    "    return float(num_pix_nonzero) / float(num_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def green_median(img):\n",
    "    h,w = img.shape[:2]\n",
    "    \n",
    "    g = img[:,:,1]\n",
    "    med = np.median(g)\n",
    "    \n",
    "    return med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(img):\n",
    "    entro = shannon_entropy(img)\n",
    "    return entro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glcm_entropy(img):\n",
    "    #bw_img = rgb2grey(img)\n",
    "    #mask = cv2.threshold(bw_img, 127, 255, cv2.THRESH_OTSU)\n",
    "    #b,g,r = cv2.split(img)\n",
    "    #ex_g = 2.0 * g - b - r\n",
    "    #ex_g = ex_g.astype(\"uint8\")\n",
    "    #ex_g = cv2.cvtColor(ex_g, cv2.COLOR_GRAY2RGB)\n",
    "    #ex_g = cv2.cvtColor(ex_g, cv2.COLOR_RGB2GRAY)\n",
    "    #print(ex_g.shape)\n",
    "    #print(ex_g.dtype)\n",
    "    \n",
    "    mask = cv2.threshold(img, 127, 255, cv2.THRESH_OTSU)\n",
    "    #mask = cv2.threshold(ex_g, 127, 255, cv2.THRESH_OTSU)\n",
    "    #glcm = greycomatrix(mask,10,0, normed=True)\n",
    "    glcm = greycomatrix(img,[10],[0, np.pi/2], normed=True, symmetric=True)\n",
    "    #print(glcm)\n",
    "    \n",
    "    contrast = greycoprops(glcm, 'contrast')[0][0]\n",
    "    dissimilarity = greycoprops(glcm, 'dissimilarity')[0]\n",
    "    homogeneity = greycoprops(glcm, 'homogeneity')[0]\n",
    "    energy = greycoprops(glcm, 'energy')[0]\n",
    "    correlation = greycoprops(glcm, 'correlation')[0]\n",
    "    asm = greycoprops(glcm, 'ASM')[0]\n",
    "    \n",
    "    return (contrast, dissimilarity, homogeneity, energy, correlation, asm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_edge_effects(plot_img):\n",
    "    h,w = plot_img.shape[:2]\n",
    "    h_edge = 0\n",
    "    w_edge = 0\n",
    "    if h > w:\n",
    "        h_edge = int(h * 0.15)\n",
    "        w_edge = int(w * 0.1)\n",
    "    else:\n",
    "        h_edge = int(h * 0.1)\n",
    "        w_edge = int(w * 0.15)\n",
    "    \n",
    "    return plot_img[0+h_edge:h-h_edge,0+w_edge:w-w_edge,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_height(img):\n",
    "    if len(img.shape) > 2:\n",
    "        plot_img = img[:,:,0]\n",
    "    else:\n",
    "        plot_img = img\n",
    "    area = plot_img.shape[0] * plot_img.shape[1]\n",
    "    count = 0\n",
    "    for x in range(plot_img.shape[0]):\n",
    "        for y in range(plot_img.shape[1]):\n",
    "            if plot_img[x,y] == 0:\n",
    "                count += 1\n",
    "    plot_img = plot_img.ravel()[np.flatnonzero(plot_img)]\n",
    "    \n",
    "    # Here, if there is not enough data from the point cloud I will return None\n",
    "    # to indicate that there wasn't enough data for the plot in question\n",
    "    if len(plot_img) / float(area) < 0.5:\n",
    "        return None\n",
    "    \n",
    "    height = np.mean(plot_img)\n",
    "    height = height / 255.0\n",
    "    \n",
    "    return height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(img):\n",
    "    plot = remove_edge_effects(img)\n",
    "    veg_i = vegetative_index(plot)\n",
    "    aniso = anisotropy(plot)\n",
    "    cover = coverage(plot)\n",
    "    green = green_median(plot)\n",
    "    entro = entropy(plot)\n",
    "    return (veg_i, aniso[1], cover, green, entro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_coords(coords, start_shape, target_shape):\n",
    "    (x,y,w,h,a) = coords\n",
    "    \n",
    "    x = float(x) / float(start_shape[1])\n",
    "    x = int(x * target_shape[1])\n",
    "    y = float(y) / float(start_shape[0])\n",
    "    y = int(y * target_shape[0])\n",
    "    w = float(w) / float(start_shape[1])\n",
    "    w = int(w * target_shape[1])\n",
    "    h = float(h) / float(start_shape[0])\n",
    "    h = int(h * target_shape[0])\n",
    "    a = w * h\n",
    "\n",
    "    return (x,y,w,h,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_coords_on_img(img,r,c,x,y,w,h):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    string = str(r) + \":\" + str(c)\n",
    "    cv2.putText(img,string,(int(x+w/3),int(y+h/2)), font,1, (0,0,255),2,cv2.LINE_AA)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2474, 626, 142, 46, 6532)\n",
      "(46, 142, 3)\n"
     ]
    }
   ],
   "source": [
    "#lab_img = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "\n",
    "test_plot = plot2[200]\n",
    "print(test_plot)\n",
    "(x,y,w,h,a) = test_plot\n",
    "test_plot_img = img[y:y+h,x:x+w]\n",
    "print(test_plot_img.shape)\n",
    "test_data = get_data(test_plot_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1949, 4045, 3)\n",
      "714\n"
     ]
    }
   ],
   "source": [
    "csv_data = []\n",
    "coord_img = hmap.copy()\n",
    "print(coord_img.shape)\n",
    "for plot in plots_with_grids:\n",
    "    (r,c,x,y,w,h,a) = plot\n",
    "    (x,y,w,h,a) = convert_coords((x,y,w,h,a),img.shape[:2],hmap.shape[:2]) # Uncomment when using hmap\n",
    "    coord_img = print_coords_on_img(coord_img,r,c,x,y,w,h)\n",
    "\n",
    "    plot_img = hmap[y:y+h,x:x+w]\n",
    "    plot_img = remove_edge_effects(plot_img)\n",
    "    veg_i, aniso, cover, green, entro = get_data(plot_img)\n",
    "    \n",
    "    #(r,c,x,y,w,h,a) = plot\n",
    "    #(x,y,w,h,a) = convert_coords((x,y,w,h,a),img.shape[:2],hmap.shape[:2])\n",
    "    #hplot = remove_edge_effects(hmap[y:y+h,x:x+w])\n",
    "    #height = get_height(hplot)\n",
    "    \n",
    "    # For initial good img\n",
    "    #csv_data.append((r,c,veg_i, aniso, cover, green, entro, height))\n",
    "    # For image with grid overlaid\n",
    "    csv_data.append((r,c,veg_i, aniso, cover, green, entro))\n",
    "    # For heatmap with grid overlaid\n",
    "    #csv_data.append((r,c,height))\n",
    "    \n",
    "    \n",
    "\n",
    "print(len(csv_data))\n",
    "#cv2.imwrite(\"index.png\",coord_img)\n",
    "#pickle_name = \"180515rres.pickle\"\n",
    "#pickle.dump(plot2,open(pickle_name,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot0 = plots_with_grids[21]\n",
    "(r,c,x,y,w,h,a) = plot0\n",
    "(x,y,w,h,a) = convert_coords((x,y,w,h,a), img.shape[:2], hmap.shape[:2])\n",
    "hplot = remove_edge_effects(hmap[y:y+h,x:x+w])\n",
    "height = get_height(hplot)\n",
    "cv2.imwrite(\"test_plot.png\",hmap[y:y+h,x:x+w])\n",
    "#print(height)\n",
    "#with open('test.csv','w') as csvfile:\n",
    "#    data_w = csv.writer(csvfile)\n",
    "#    for row in hplot:\n",
    "#        data_w.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 160.99011680143755, 0.16557881148172454, 0.8230008984725966, 128.0, 7.223961144366756)\n"
     ]
    }
   ],
   "source": [
    "print(csv_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_name = \"dfw_18_07_23\"\n",
    "dir_name = \"dfw_18_07_09_h\"\n",
    "with open(dir_name+\".csv\",'w') as csvfile:\n",
    "    data_w = csv.writer(csvfile)\n",
    "    # Row IDX - row\n",
    "    # Column IDX - column\n",
    "    # Veg Greenness IDX - vegetative index\n",
    "    # Canopy Orientation - Isotropy (high numbers indicates likely lodging)\n",
    "    # Canopy Structure - Shannon Entropy score\n",
    "    # Greenness Reading - Median value of green channel\n",
    "    # Relative height - after further work we can attempt to give an absolute value\n",
    "    data_w.writerow(['Row IDX','Column IDX','Veg. Greenness IDX','Canopy Orientation','Coverage','Greenness Reading','Canopy Structure','Relative Height'])\n",
    "    for row in csv_data:\n",
    "        string = []\n",
    "        for item in row:\n",
    "            if item is not None:\n",
    "                string.append(\"%.3f\" % item if not float(item).is_integer() else item)\n",
    "            else:\n",
    "                string.append(\"N/A\")\n",
    "        data_w.writerow(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2338, 4637, 3)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = \"/Users/bauera/work/airsurf/air_surf_wheat_cnn/dfw_18_06_13_intermediates/output.png\"\n",
    "step1 = cv2.imread(step1)\n",
    "step2 = \"/Users/bauera/work/airsurf/air_surf_wheat_cnn/dfw_18_06_13_intermediates/houghlines.png\"\n",
    "step2 = cv2.imread(step2)\n",
    "step3 = \"/Users/bauera/work/airsurf/air_surf_wheat_cnn/dfw_18_06_13_intermediates/plots.png\"\n",
    "step3 = cv2.imread(step3)\n",
    "\n",
    "stacked = np.hstack((step1,step2,step3))\n",
    "stacked = cv2.resize(stacked,(int(stacked.shape[1]/10),int(stacked.shape[0]/10)), interpolation=cv2.INTER_AREA)\n",
    "cv2.imwrite(\"stacked.png\",stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [\n",
    "    \"/Users/bauera/work/airsurf/wheat/textures/circular.png\",\n",
    "    \"/Users/bauera/work/airsurf/wheat/textures/cross-hatched.png\",\n",
    "    \"/Users/bauera/work/airsurf/wheat/textures/horizontal.png\",\n",
    "    \"/Users/bauera/work/airsurf/wheat/textures/isotropic.png\",\n",
    "    \"/Users/bauera/work/airsurf/wheat/textures/radial.png\",\n",
    "    \"/Users/bauera/work/airsurf/wheat/textures/vertical.png\"\n",
    "]\n",
    "test_images = [\n",
    "    \"../textures/test0.png\",\n",
    "    \"../textures/test1.png\",\n",
    "    \"../textures/test2.png\",\n",
    "    \"../textures/test3.png\",\n",
    "    \"../textures/test4.png\",\n",
    "    \"../textures/test5.png\",\n",
    "    \"../textures/test6.png\"    \n",
    "]\n",
    "test_images_names = [\"circular\", \"cross-hatched\", \"horizontal\", \"isotropic\", \"radial\", \"vertical\"]\n",
    "test_images_names = [\"vertical\", \"horizontal\", \"diagonal_left\", \"diagonal_right\", \"checkerboard\", \"cross-hatch\",\"road\"]\n",
    "test_data = []\n",
    "\n",
    "for image in test_images:\n",
    "    img = cv2.imread(image,0) # 0 is for grayscale\n",
    "    #img = np.asarray(img,dtype='uint8')\n",
    "    info = glcm_entropy(img)\n",
    "    test_data.append(info)\n",
    " \n",
    "for i in range(len(test_images)):\n",
    "    print(test_images_names[i] + \":\")\n",
    "    print(test_data[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65025.0, array([255.,   0.]), array([1.5378464e-05, 1.0000000e+00]), array([0.70710678, 0.70710678]), array([-1.,  1.]), array([0.5, 0.5]))\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uint8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Test images\n",
    "test_img = np.zeros((100,100),'uint8')\n",
    "print(test_img.dtype)\n",
    "for row in range(100):\n",
    "    for col in range(100):\n",
    "        if ((row - col) // 10) % 2 == 0:\n",
    "            test_img[row][col] = 255\n",
    "\n",
    "cv2.imwrite(\"../textures/test5.png\", test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 100)\n",
      "uint8\n",
      "(100, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = cv2.imread(\"../textures/test0.png\",0)\n",
    "img2 = cv2.imread(\"../textures/test1.png\",0)\n",
    "print(img1.shape)\n",
    "print(img1.dtype)\n",
    "print(img2.shape)\n",
    "img3 = np.zeros((100,100), 'uint8')\n",
    "for row in range(100):\n",
    "    for col in range(100):\n",
    "        if (img1[row][col] == 255) and (img2[row][col] == 255):\n",
    "            img3[row][col] = 255\n",
    "\n",
    "cv2.imwrite(\"../textures/test6.png\", img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
