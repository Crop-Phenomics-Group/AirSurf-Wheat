{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "#                                                               #\n",
    "#    Author: Alan Bauer, alan.bauer@earlham.ac.uk               #\n",
    "#    Date: 2018.11.15, V0.1                                     #\n",
    "#    Version: 0.1 on Bitbucket                                  #\n",
    "#                                                               #\n",
    "#################################################################\n",
    "\n",
    "# Attempting to segment aerial wheat images into individual plots\n",
    "# and extract data from each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import sgd\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "import keras\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process into 9x9 images. It assumes that if they aren't 9x9 they are 11x11\n",
    "# because that is the size I first started classifying. In that case the\n",
    "# outermost pixels are removed so the center pixel remains the same.\n",
    "def preprocess(img):\n",
    "    h,w = img.shape[:2]\n",
    "    if h is 9 and w is 9:\n",
    "        return img\n",
    "    \n",
    "    up_img = img[1:10,1:10]\n",
    "\n",
    "    return up_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 9, 3)\n"
     ]
    }
   ],
   "source": [
    "# Just testing that the preprocess function properly resizes images\n",
    "img = \"datasets/soil/soil/0_0_04_28.png\"\n",
    "img = cv2.imread(img)\n",
    "up_img = preprocess(img)\n",
    "cv2.imwrite(\"test.png\",up_img)\n",
    "print(up_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images from a base directory that contains two subfolders,\n",
    "# one for each of the two classes\n",
    "def load_imgs(base_path):\n",
    "    pos_class = \"soil\"\n",
    "    neg_class = \"not_soil\"\n",
    "    \n",
    "    files = []\n",
    "    ext = \".png\" # May need to make this more comprehensive on a different OS\n",
    "    \n",
    "    data = []\n",
    "    labels = []\n",
    "    \n",
    "    for (top_dir, dirs, filenames) in os.walk(base_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(ext):\n",
    "                f = os.path.join(top_dir,filename)\n",
    "                img = cv2.imread(f)\n",
    "                img = preprocess(img)\n",
    "                data.append(img)\n",
    "\n",
    "                label = f.split(os.path.sep)[-2]\n",
    "                labels.append(label)\n",
    "    \n",
    "    return (np.array(data), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the CNN\n",
    "def build_model(width, height, depth, classes):\n",
    "    model = Sequential()\n",
    "    shape = (height, width, depth)\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        shape = (depth, height, width)\n",
    "    \n",
    "    model.add(Conv2D(32, (3,3), padding=\"same\", activation=\"relu\", input_shape=shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation=\"relu\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(classes,activation=\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3664\n"
     ]
    }
   ],
   "source": [
    "# Load the data and split it into training and testing sets\n",
    "data_dir = \"datasets/soil/\"\n",
    "(data, labels) = load_imgs(data_dir)\n",
    "data = data.astype(\"float\") / 255.0\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=1)\n",
    "\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "trainY = np_utils.to_categorical(trainY)\n",
    "testY = np_utils.to_categorical(testY)\n",
    "\n",
    "print(len(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = build_model(9,9,3,2)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network\n",
      "Train on 2748 samples, validate on 916 samples\n",
      "Epoch 1/100\n",
      "2748/2748 [==============================] - 12s 4ms/step - loss: 0.2922 - acc: 0.9079 - val_loss: 0.3342 - val_acc: 0.8897\n",
      "Epoch 2/100\n",
      "2748/2748 [==============================] - 10s 3ms/step - loss: 0.1960 - acc: 0.9385 - val_loss: 0.3746 - val_acc: 0.8941\n",
      "Epoch 3/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.1548 - acc: 0.9487 - val_loss: 0.2915 - val_acc: 0.9159\n",
      "Epoch 4/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.1249 - acc: 0.9592 - val_loss: 0.0752 - val_acc: 0.9760\n",
      "Epoch 5/100\n",
      "2748/2748 [==============================] - 8s 3ms/step - loss: 0.1218 - acc: 0.9556 - val_loss: 0.1930 - val_acc: 0.9531\n",
      "Epoch 6/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.1133 - acc: 0.9643 - val_loss: 0.1179 - val_acc: 0.9651\n",
      "Epoch 7/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0904 - acc: 0.9709 - val_loss: 0.1751 - val_acc: 0.9531\n",
      "Epoch 8/100\n",
      "2748/2748 [==============================] - 8s 3ms/step - loss: 0.0858 - acc: 0.9698 - val_loss: 0.1138 - val_acc: 0.9694\n",
      "Epoch 9/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0751 - acc: 0.9745 - val_loss: 0.0966 - val_acc: 0.9672\n",
      "Epoch 10/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0783 - acc: 0.9760 - val_loss: 0.0870 - val_acc: 0.9738\n",
      "Epoch 11/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0605 - acc: 0.9745 - val_loss: 0.0706 - val_acc: 0.9716\n",
      "Epoch 12/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0530 - acc: 0.9807 - val_loss: 0.0889 - val_acc: 0.9749\n",
      "Epoch 13/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0544 - acc: 0.9803 - val_loss: 0.1247 - val_acc: 0.9531\n",
      "Epoch 14/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0434 - acc: 0.9869 - val_loss: 0.2088 - val_acc: 0.9465\n",
      "Epoch 15/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0451 - acc: 0.9869 - val_loss: 0.1844 - val_acc: 0.9498\n",
      "Epoch 16/100\n",
      "2748/2748 [==============================] - 9s 3ms/step - loss: 0.0544 - acc: 0.9807 - val_loss: 0.0927 - val_acc: 0.9716\n",
      "Epoch 17/100\n",
      "2748/2748 [==============================] - 8s 3ms/step - loss: 0.0450 - acc: 0.9844 - val_loss: 0.0839 - val_acc: 0.9771\n",
      "Epoch 18/100\n",
      "2112/2748 [======================>.......] - ETA: 1s - loss: 0.0426 - acc: 0.9853"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-280769138216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] training network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[INFO] evaluating network\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deep_learning/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model, if you are loading a model from disk, skip this step\n",
    "print(\"[INFO] training network\")\n",
    "\n",
    "H = model.fit(trainX, trainY, validation_data=(testX,testY),batch_size=32, epochs=100, verbose=1)\n",
    "\n",
    "print(\"[INFO] evaluating network\")\n",
    "predictions = model.predict(testX, batch_size=32,verbose=1)\n",
    "target_names = [\"not soil\",\"soil\"]\n",
    "print(classification_report(testY.argmax(axis=1),predictions.argmax(axis=1),target_names=target_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save or load model\n",
    "\n",
    "#model.save(\"models/soil/model_5.h5\")\n",
    "model = load_model(\"models/soil/model_5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 9, 9, 32)          896       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 9, 9, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 9, 9, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 9, 9, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               1049088   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 1,146,306\n",
      "Trainable params: 1,144,834\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Info about model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This extracts 9x9 images from a larger image and returns\n",
    "# 4 numpy arrays, the images, an empty array for labels to be added\n",
    "# and the coordinates of the top left corner of the images\n",
    "def get_small_imgs_from_mosaic(img):\n",
    "    h,w = img.shape[:2]\n",
    "    step = 6\n",
    "    size = 9\n",
    "        \n",
    "    step_x = 5\n",
    "    step_y = 5\n",
    "    \n",
    "    images_to_pred = []\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    \n",
    "    for x in range(0,w-size,step):\n",
    "        for y in range(0,h-size,step):\n",
    "            small_img = img[y:y+size,x:x+size]\n",
    "            small_img = small_img.astype(\"float\") / 255.0\n",
    "            images_to_pred.append(small_img)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "    \n",
    "    labels = [None] * len(X)\n",
    "    \n",
    "    return (np.array(images_to_pred),np.array(labels),np.array(X),np.array(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134980\n"
     ]
    }
   ],
   "source": [
    "# What images should we read in for segmentation\n",
    "img = cv2.imread(\"aaron_ROI_test_original/2016_05_19.png\")\n",
    "#img = cv2.imread(\"20mb_pngs/Morley_180611.png\")\n",
    "(images, labels, x_values, y_values) = get_small_imgs_from_mosaic(img)\n",
    "\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model on the small images extracted from the original\n",
    "outputs = model.predict(images,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draws the areas classified as soil with >98% confidence on\n",
    "# the original image as well as a black and white mask and\n",
    "# saves them to disk\n",
    "h,w = img.shape[:2]\n",
    "size = 9\n",
    "step = 6\n",
    "out_img = img.copy()\n",
    "out_img_bw = np.zeros((h,w))\n",
    "\n",
    "for i in range(len(outputs)):\n",
    "    if outputs[i][1] >= 0.98:\n",
    "        x = x_values[i]\n",
    "        y = y_values[i]\n",
    "        cv2.rectangle(out_img, (x+1,y+1),(x+size-1,y+size-1),(0,0,255),-1)\n",
    "        cv2.rectangle(out_img_bw,(x+1,y+1),(x+size-1,y+size-1),255,-1)\n",
    "        #out_img_bw[y+1:y+size-1][x+1:x+size-1] = 1\n",
    "        #out_img_bw[x+1:x+size-1][y+1:y+size-1] = 1\n",
    "        \n",
    "cv2.imwrite(\"output.png\",out_img)\n",
    "\n",
    "out_img_bw = out_img_bw * 255\n",
    "out_img_bw.astype(\"uint8\")\n",
    "#kernel = np.ones((11,11))\n",
    "#out_img_bw =cv2.dilate(out_img_bw,kernel,1)\n",
    "#out_img_bw = cv2.morphologyEx(out_img_bw, cv2.MORPH_CLOSE, kernel)\n",
    "cv2.imwrite(\"output_bw.png\",out_img_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combines lines that are too close together, ideally\n",
    "# resulting in only 1 line between plots.\n",
    "def line_consensus(lines):\n",
    "    lines.sort()\n",
    "    cons = []\n",
    "    #print(lines)\n",
    "\n",
    "    for i in range(len(lines)-1):\n",
    "        if lines[i+1] - lines[i] < 10:\n",
    "            avg = (lines[i+1] + lines[i]) / 2\n",
    "            cons.append(avg)\n",
    "        # Special case for item first in list\n",
    "        elif lines[i] - lines[i-1] >= 10 and i != 0:\n",
    "            #continue\n",
    "            cons.append(lines[i])\n",
    "        elif i == 0:\n",
    "            cons.append(lines[i])\n",
    "    \n",
    "    # Special case for item last in list\n",
    "    if lines[-1] - lines[-2] >= 10:\n",
    "        cons.append(lines[-1])\n",
    "    \n",
    "    return cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a mask image from the x and y coordinates of\n",
    "# vertical and horizontal lines, respectively.\n",
    "def mask_write(shape,ver_cons,hor_cons,name):\n",
    "    mask = np.zeros(shape)\n",
    "    \n",
    "    # Vertical lines\n",
    "    for line in ver_cons:\n",
    "        x = int(line)\n",
    "        y0 = 0\n",
    "        y1 = shape[0]\n",
    "        cv2.line(mask,(x,y0),(x,y1),255,1)\n",
    "    \n",
    "    for line in hor_cons:\n",
    "        y = int(line)\n",
    "        x0 = 0\n",
    "        x1 = shape[1]\n",
    "        cv2.line(mask,(x0,y),(x1,y),255,1)\n",
    "    \n",
    "    cv2.imwrite(name,mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because the fields have small plots that make it so the\n",
    "# lines between plots are not uniformly spaced horizontally\n",
    "# (on these images, it could work the other way too) I\n",
    "# wanted to see if finding the edge lines and then simply\n",
    "# equalizing the distance between all the intervening lines\n",
    "# would work. It didn't. Either the plots are not exactly\n",
    "# the same width, or pixel width is not uniform across the\n",
    "# entire image.\n",
    "def vert_equalize(lines):\n",
    "    distances = []\n",
    "    ret_lines = []\n",
    "    for i in range(len(lines)-1):\n",
    "        distances.append(lines[i+1] - lines[i])\n",
    "    \n",
    "    print(np.mean(distances))\n",
    "    mean = np.mean(distances)\n",
    "    least = lines[0]\n",
    "    most = lines[-1]\n",
    "    for i in range(len(lines)):\n",
    "        if i == 0:\n",
    "            ret_lines.append(least)\n",
    "        elif i == len(lines)-1:\n",
    "            ret_lines.append(most)\n",
    "        else:\n",
    "            ret_lines.append(least + mean * i)\n",
    "    \n",
    "    return ret_lines\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(170, 1, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw all the Hough Lines on the image.\n",
    "# Save a black and white mask, and the original\n",
    "# with lines drawn on it.\n",
    "out = img.copy()\n",
    "#cv2.imwrite(\"test.png\",out)\n",
    "### Transition to using Hough lines\n",
    "#out_img_bw = cv2.imread(\"output_bw.png\")\n",
    "mask = out_img_bw\n",
    "hough_bw = np.zeros((h,w))\n",
    "#edges = cv2.Canny(out_img,50,150,aperture_size=3)\n",
    "lines = cv2.HoughLines(mask.astype(\"uint8\"),1,np.pi/90,1500)\n",
    "#print(lines.shape)\n",
    "for line in lines:\n",
    "    #print(line)\n",
    "    for rho,theta in line:\n",
    "        a = np.cos(theta)\n",
    "        b = np.sin(theta)\n",
    "        x0 = a*rho\n",
    "        y0 = b*rho\n",
    "        x1 = int(x0 + 10000*(-b))\n",
    "        y1 = int(y0 + 10000*(a))\n",
    "        x2 = int(x0 - 10000*(-b))\n",
    "        y2 = int(y0 - 10000*(a))\n",
    "    \n",
    "        cv2.line(out,(x1,y1),(x2,y2),(0,0,255),5)\n",
    "        cv2.line(hough_bw,(x1,y1),(x2,y2),255,5)\n",
    "\n",
    "\n",
    "hough_bw = np.bitwise_not(hough_bw.astype(\"uint8\"))\n",
    "\n",
    "cv2.imwrite('houghlines.png', out)\n",
    "cv2.imwrite('houghlines_bw.png', hough_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41.44897959183673\n",
      "(2390, 2045, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the Hough lines into horizontal and vertical lines\n",
    "\n",
    "### I want to combine the nearby lines rather than just making them thicker.\n",
    "hor = []\n",
    "ver = []\n",
    "\n",
    "for line in lines:\n",
    "    if line[0][1] < 0.1:\n",
    "        ver.append(line[0,0])\n",
    "    else:\n",
    "        hor.append(line[0,0])\n",
    "\n",
    "hor.sort()\n",
    "ver.sort()\n",
    "ver2 = ver\n",
    "hor2 = hor\n",
    "hor_cons = []\n",
    "ver_cons = []\n",
    "\n",
    "while len(hor_cons) != len(hor):\n",
    "    hor_cons = hor\n",
    "    hor = line_consensus(hor)\n",
    "    \n",
    "while len(ver_cons) != len(ver):\n",
    "    ver_cons = ver\n",
    "    ver = line_consensus(ver)\n",
    "    \n",
    "ver_eq = vert_equalize(ver_cons)\n",
    "#Making the horizontal distances equal is not as accurate, because the plots are not all perfectly the same size.\n",
    "#print(len(ver_cons))\n",
    "#print(len(ver_eq))\n",
    "\n",
    "\n",
    "# Write out various images using different sets of lines.\n",
    "mask = mask_write((h,w),ver_cons,hor_cons,\"test0.png\")\n",
    "mask = np.bitwise_not(mask.astype(\"uint8\"))\n",
    "cv2.imwrite(\"mask2.png\",mask)\n",
    "mask_write((h,w),ver_cons,hor_cons,\"test.png\")\n",
    "mask_write((h,w),ver2,hor2,\"test2.png\")\n",
    "\n",
    "cutout = img.copy()\n",
    "output = cv2.bitwise_and(cutout,cutout,mask=mask)\n",
    "print(cutout.shape)\n",
    "cv2.imwrite(\"test3.png\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2390, 2045, 3)\n",
      "(2390, 2045)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write an image that should only include the areas that are in plots, but will also include small plots.\n",
    "cutout = img\n",
    "\n",
    "output = cv2.bitwise_and(cutout,cutout,mask=hough_bw)\n",
    "print(cutout.shape)\n",
    "print(hough_bw.shape)\n",
    "cv2.imwrite(\"cutout.png\",output)\n",
    "\n",
    "#b,g,r = cv2.split(img)\n",
    "#b = b & hough_bw\n",
    "#g = g & hough_bw\n",
    "#r = r & hough_bw\n",
    "\n",
    "#cutouts = cv2.merge(b,g)\n",
    "#cutouts = cv2.merge(cutouts,r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "### Extract plots  49x16\n",
    "# I was using hard-coded values to see about how big the area of each plot is\n",
    "# but it should be detected automatically, using the numbers of rows\n",
    "# and columns.\n",
    "plots = []\n",
    "\n",
    "im2,contours,hierarchy = cv2.findContours(mask,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "for contour in contours:\n",
    "    x,y,width,height = cv2.boundingRect(contour)\n",
    "    area = width*height\n",
    "    plots.append((x,y,width,height,area))\n",
    "\n",
    "areas = []\n",
    "for plot in plots:\n",
    "    areas.append(plot[4])\n",
    "\n",
    "area_set = list(set(areas))\n",
    "hist = []\n",
    "#print(area_set)\n",
    "\n",
    "for i in area_set:\n",
    "    count = 0\n",
    "    for area in areas:\n",
    "        if area == i:\n",
    "            count += 1\n",
    "    \n",
    "    hist.append(count)\n",
    "\n",
    "#plt.bar(hist,area_set)\n",
    "#plt.bar(area_set,hist)\n",
    "#plt.show()\n",
    "\n",
    "plot2 = [plot for plot in plots if plot[4] > 2100]\n",
    "print(len(plot2))\n",
    "#49*16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create images that are the final mask, as well as the original image\n",
    "# with all the non-plot regions removed.\n",
    "final_mask = np.zeros(hough_bw.shape)\n",
    "for plot in plot2:\n",
    "    final_mask[plot[1]:plot[1]+plot[3],plot[0]:plot[0]+plot[2]] = 1\n",
    "\n",
    "\n",
    "plots_img = cv2.bitwise_and(cutout,cutout,mask=final_mask.astype(\"uint8\"))    \n",
    "\n",
    "cv2.imwrite(\"plots.png\",plots_img)\n",
    "cv2.imwrite(\"final_mask.png\",final_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each plot image in a folder, with the row and column\n",
    "# information in its filename.\n",
    "# TODO: This will be changed to saving each plot in a unique\n",
    "# folder, and then saving images from different days together.\n",
    "if not os.path.exists(\"unique_plots\"):\n",
    "    os.mkdir(\"unique_plots\")\n",
    "    \n",
    "if plot2[0][0] > plot2[-1][0]:\n",
    "    plot2 = list(reversed(plot2))\n",
    "\n",
    "grid_row = 0\n",
    "grid_col = 0\n",
    "raw_x = plot2[0][1]\n",
    "    \n",
    "for plot in plot2:\n",
    "    if plot[1] != raw_x:\n",
    "        grid_row += 1\n",
    "        grid_col = 0\n",
    "        raw_x = plot[1]\n",
    "    name = \"unique_plots/\" + str(grid_row) + \"_\" + str(grid_col) + \".png\"\n",
    "    cv2.imwrite(name,plots_img[plot[1]:plot[1]+plot[3],plot[0]:plot[0]+plot[2]])\n",
    "    grid_col += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the weighted centroid of the plot based on greenness,\n",
    "# which should be the center of the plot.\n",
    "# We can then extrapolate that information to images later in\n",
    "# a series.\n",
    "\n",
    "# NOT FINISHED\n",
    "def centroid(points):\n",
    "    total = 0\n",
    "    weighted_total = 0\n",
    "    for i in range(len(points)):\n",
    "        weighted_total += points[i]*(i+1)\n",
    "        total += points[i]\n",
    "\n",
    "    mean = total / len(points)\n",
    "    centroid = weighted_total / len(points) / mean\n",
    "    return centroid - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Testing code\n",
    "points = []\n",
    "print (centroid(points))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
